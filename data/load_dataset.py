import os
import pandas as pd
import pickle

class _NumpyCoreAliasUnpickler(pickle.Unpickler):
    def find_class(self, module, name):
        if module.startswith("numpy._core"):
            module = module.replace("numpy._core", "numpy.core")
        return super().find_class(module, name)

def read_pickle_compat(path):
    import pandas as pd
    try:
        return pd.read_pickle(path)
    except ModuleNotFoundError as e:
        if "numpy._core" not in str(e):
            raise
        with open(path, "rb") as fh:
            return _NumpyCoreAliasUnpickler(fh).load()
import numpy as np
from typing import List, Tuple, Optional, Dict
from pytorch_forecasting import TimeSeriesDataSet
from pytorch_forecasting.data import NaNLabelEncoder, MultiNormalizer, TorchNormalizer
from torch.utils.data import DataLoader
from utils.feature_utils import get_pinned_features
import re


def _robust_parse_datetime(df: pd.DataFrame) -> pd.DataFrame:
    """Ensure a valid 'datetime' column by robust parsing.

    - Tries pandas>=2.0 format='mixed' with errors='coerce'
    - Falls back to generic to_datetime coercion
    - If still poor, tries using a numeric 'timestamp' (auto-detect ms vs s)
    """
    if 'datetime' in df.columns:
        s = df['datetime']
        parsed = None
        try:
            parsed = pd.to_datetime(s, format='mixed', errors='coerce')
        except TypeError:
            parsed = pd.to_datetime(s, errors='coerce', infer_datetime_format=False)
        if parsed.notna().any():
            df['datetime'] = parsed
            return df

    if 'timestamp' in df.columns:
        ts = pd.to_numeric(df['timestamp'], errors='coerce')
        if ts.notna().any():
            med = float(ts.dropna().median()) if ts.dropna().size else 0.0
            unit = 'ms' if med >= 1e11 else 's'
            parsed = pd.to_datetime(ts, unit=unit, errors='coerce')
            if parsed.notna().any():
                df['datetime'] = parsed
                return df

    # Last resort: coerce whatever is in 'datetime'
    if 'datetime' in df.columns:
        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
        return df
    raise ValueError("Cannot construct a valid 'datetime' column from either 'datetime' or 'timestamp'.")


def get_dataloaders(
    data_path: str,
    batch_size: int = 64,
    num_workers: int = 4,
    val_mode: str = "days",
    val_days: int = 30,
    val_ratio: float = 0.2,
    targets_override: Optional[List[str]] = None,
    periods: Optional[List[str]] = None,
    symbols: Optional[List[str]] = None,
    selected_features_path: Optional[str] = None,
    pinned_features_cfg: Optional[Dict[str, List[str]]] = None,
    modality: Optional[str] = None,
    **kwargs,
) -> Tuple[DataLoader, DataLoader, List[str], TimeSeriesDataSet, List[str], dict, dict]:
    # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ data_pathï¼›è‹¥ä¸å¯ç”¨åˆ™å›žé€€åˆ°é»˜è®¤ pkl è·¯å¾„
    default_pkl = "data/pkl_merged/full_merged.pkl"
    use_path = data_path or default_pkl
    if not os.path.exists(use_path):
        use_path = default_pkl
    df = read_pickle_compat(use_path)

    df = df.astype({
        **{col: "float32" for col in df.select_dtypes(include="float64").columns},
        **{col: "int32" for col in df.select_dtypes(include="int64").columns},
    })
    df = _robust_parse_datetime(df)
    # å¯é€‰ï¼šæŒ‰ä¸“å®¶é…ç½®è¿‡æ»¤å‘¨æœŸä¸Žç¬¦å·
    if periods:
        pset = set(str(p) for p in periods)
        df = df[df["period"].astype(str).isin(pset)]
    if symbols:
        sset = set(str(s) for s in symbols)
        df = df[df["symbol"].astype(str).isin(sset)]

    # ä¸¥æ ¼åŽ»æŽ‰åˆ†ç»„é”®ç¼ºå¤±çš„è¡Œï¼Œé¿å…å‡ºçŽ°å­—ç¬¦ä¸² "nan" è¢«å½“æˆåˆæ³•ç±»åˆ«
    df = df[df["symbol"].notna() & df["period"].notna()].copy()
    # è§„èŒƒåŒ– symbolï¼šåŽ»ç©ºæ ¼/ç»Ÿä¸€å¤§å†™/åˆ†éš”ç¬¦æ ‡å‡†åŒ–ï¼›å°† BTCUSDT ä¹‹ç±»åˆå¹¶ä¸º BTC_USDT
    def _canon_symbol(x: str) -> str:
        s = str(x).strip().upper()
        s = s.replace('/', '_').replace('-', '_')
        m = re.match(r"^([A-Z]+)(USDT|USD|BUSD|USDC)$", s)
        if m:
            s = f"{m.group(1)}_{m.group(2)}"
        return s
    try:
        df['symbol'] = df['symbol'].astype(str).map(_canon_symbol)
    except Exception:
        pass
    df = df.sort_values(["symbol", "period", "datetime"]).copy()
    df["time_idx"] = df.groupby(["symbol", "period"]).cumcount()
    df["symbol"] = df["symbol"].astype(str)
    df["period"] = df["period"].astype(str)

    numeric_cols = [
        c
        for c in df.columns
        if np.issubdtype(df[c].dtype, np.number)
        and c not in {"time_idx"}
    ]
    nan_fill_summary = {
        "total_nan_before": int(df[numeric_cols].isna().sum().sum()) if numeric_cols else 0,
        "zero_filled_columns": [],
    }
    if numeric_cols:
        df[numeric_cols] = df.groupby(["symbol", "period"], group_keys=False)[numeric_cols].ffill()
        df[numeric_cols] = df.groupby(["symbol", "period"], group_keys=False)[numeric_cols].bfill()
        remaining_nan = df[numeric_cols].isna().sum()
        nan_fill_summary["total_nan_after_ffill_bfill"] = int(remaining_nan.sum())
        still_missing_cols = remaining_nan[remaining_nan > 0].index.tolist()
        if still_missing_cols:
            df[still_missing_cols] = df[still_missing_cols].fillna(0.0)
            nan_fill_summary["zero_filled_columns"] = still_missing_cols
            nan_fill_summary["total_nan_after_fillna"] = int(df[numeric_cols].isna().sum().sum())
        else:
            nan_fill_summary["total_nan_after_fillna"] = int(remaining_nan.sum())
    else:
        nan_fill_summary["total_nan_after_ffill_bfill"] = 0
        nan_fill_summary["total_nan_after_fillna"] = 0

    # --- å…¨å±€æ—¶é—´åˆ’åˆ† ---
    if val_mode == "days":
        val_cutoff_date = df["datetime"].max() - pd.Timedelta(days=val_days)
        df_train = df[df["datetime"] <= val_cutoff_date].copy()
        df_val = df[df["datetime"] > val_cutoff_date].copy()
    elif val_mode == "ratio":
        # æ³¨æ„ï¼šå¯¹äºŽæ—¶é—´åºåˆ—ï¼ŒæŒ‰æ¯”ä¾‹çš„éšæœºåˆ’åˆ†é€šå¸¸ä¸æŽ¨èï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²ã€‚
        # è¿™é‡Œçš„å®žçŽ°æ˜¯æŒ‰æ—¶é—´é¡ºåºè¿›è¡Œçš„ï¼Œæ›´å®‰å…¨ã€‚
        train_dfs, val_dfs = [], []
        for _, group in df.groupby(["symbol", "period"]):
            val_idx = int(len(group) * (1 - val_ratio))
            train_dfs.append(group.iloc[:val_idx])
            val_dfs.append(group.iloc[val_idx:])
        df_train = pd.concat(train_dfs, ignore_index=True)
        df_val = pd.concat(val_dfs, ignore_index=True)
    else:
        raise ValueError(f"Unknown val_mode: {val_mode}")

    targets = [
        "target_binarytrend",
        "target_logreturn",
        "target_logsharpe_ratio",
        "target_breakout_count",
        "target_drawdown_prob",
        "target_max_drawdown",
        "target_pullback_prob",
        "target_return_outlier_lower_10",
        "target_return_outlier_upper_90",
        "target_sideway_detect",
        "target_trend_persistence",
    ]
    if targets_override:
        # åªä¿ç•™æ•°æ®ä¸­å­˜åœ¨çš„åˆ—
        targets = [t for t in targets_override if t in df.columns]
    regression_targets = [
        "target_logreturn",
        "target_logsharpe_ratio",
        "target_breakout_count",
        "target_max_drawdown",
        "target_trend_persistence",
    ]

    known_reals = ["time_idx"]
    df.replace({None: np.nan}, inplace=True)


    # å€™é€‰æœªçŸ¥å®žæ•°ç‰¹å¾ï¼ˆåœ¨ç­›é€‰/æ‹¼æŽ¥å‰çš„å…¨é‡å€™é€‰ï¼‰
    unknown_reals = [
        c for c in df_train.columns
        if c not in known_reals
        and c not in ["timestamp", "symbol", "time_idx", "datetime", "period"]
        and not c.startswith("future")
        # ä¸æŠŠä»»ä½• target_* åˆ—å½“ä½œç‰¹å¾ï¼Œåªå…è®¸åœ¨ targets åˆ—è¡¨ä¸­ä½¿ç”¨
        and not c.startswith("target_")
        and not df_train[c].isna().all()
    ]
    unknown_reals_before = list(unknown_reals)
    sel_path = selected_features_path or os.path.join("configs", "selected_features.txt")
    
    # åŠ¨æ€ç­›é€‰çš„ Alpha ç‰¹å¾ï¼ˆæ¥è‡ª selected_features.txtï¼‰
    allowlist = []
    if sel_path and os.path.exists(sel_path):
        with open(sel_path, "r", encoding="utf-8") as fh:
            allowlist = [ln.strip() for ln in fh if ln.strip() and not ln.strip().startswith("#")]
    # è®°å½•æ‰€ç”¨è·¯å¾„ï¼Œä¾¿äºŽæŽ’æŸ¥
    sel_path_used = sel_path if sel_path and os.path.exists(sel_path) else None

    # ä»Žé…ç½®æ³¨å…¥çš„ Pinned é™æ€ç‰¹å¾ï¼ˆæ•°æ®é›†é…ç½®ä¸­çš„é»˜è®¤å­—æ®µï¼‰
    pinned_features = []
    if pinned_features_cfg and modality:
        pinned_features = pinned_features_cfg.get(modality, [])
        # rich å’Œ comprehensive è‡ªåŠ¨ç»§æ‰¿ base
        if modality in ["rich", "comprehensive"]:
            base_pinned = pinned_features_cfg.get("base", [])
            pinned_features = list(dict.fromkeys(base_pinned + pinned_features))

    # åˆå¹¶ allowlist å’Œ pinned featuresï¼ŒåŽ»é‡
    combined_features = list(dict.fromkeys(allowlist + pinned_features))
    
    # ðŸ”§ ä¿®å¤ï¼šå§‹ç»ˆæ‰§è¡Œç‰¹å¾è¿‡æ»¤ï¼Œç¡®ä¿åªä½¿ç”¨å­˜åœ¨ä¸”æœ‰æ•ˆçš„ç‰¹å¾
    available_cols = set(df_train.columns)
    
    # ðŸŽ¯ æ™ºèƒ½å½’ä¸€åŒ–ç‰¹å¾åŒ¹é…ï¼šæ ¹æ®å½“å‰å‘¨æœŸæŽ¨æ–­æ­£ç¡®çš„çª—å£åŽç¼€
    def _smart_feature_match(feature_list, available_cols, periods_in_data=None):
        """æ™ºèƒ½åŒ¹é…ç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯å½’ä¸€åŒ–ç‰¹å¾çš„çª—å£åŽç¼€"""
        matched_features = []
        period_window_map = {"1h": 96, "4h": 56, "1d": 30}
        
        # å¦‚æžœæœ‰å‘¨æœŸä¿¡æ¯ï¼Œç¡®å®šå½“å‰ä¸»è¦å‘¨æœŸçš„çª—å£å¤§å°
        dominant_windows = []
        if periods_in_data:
            for p in periods_in_data:
                if str(p) in period_window_map:
                    dominant_windows.append(period_window_map[str(p)])
        if not dominant_windows:
            dominant_windows = [96, 56, 30]  # é»˜è®¤å°è¯•æ‰€æœ‰çª—å£
        
        for feat in feature_list:
            if feat in available_cols:
                matched_features.append(feat)
            elif "_zn" in feat or "_mm" in feat:
                # å°è¯•æ™ºèƒ½åŒ¹é…å½’ä¸€åŒ–ç‰¹å¾
                if "_zn" in feat:
                    base_feat = feat.split("_zn")[0]
                    method = "_zn"
                else:
                    base_feat = feat.split("_mm")[0]
                    method = "_mm"
                
                found = False
                for window in dominant_windows:
                    candidate = f"{base_feat}{method}{window}"
                    if candidate in available_cols:
                        matched_features.append(candidate)
                        found = True
                        break
                
                if not found:
                    print(f"  âš ï¸ å½’ä¸€åŒ–ç‰¹å¾ {feat} æœªæ‰¾åˆ°åŒ¹é…é¡¹")
            else:
                print(f"  âŒ ç‰¹å¾ä¸å­˜åœ¨: {feat}")
        
        return matched_features
    
    if combined_features:
        # ä½¿ç”¨é…ç½®æŒ‡å®šçš„ç‰¹å¾ï¼ˆæ™ºèƒ½åŒ¹é…å½’ä¸€åŒ–ç‰¹å¾ï¼‰
        current_periods = [str(p) for p in periods] if periods else None
        final_features = _smart_feature_match(combined_features, available_cols, current_periods)
        unknown_reals = [c for c in unknown_reals if c in final_features]
        print(f"âœ… ä½¿ç”¨é…ç½®ç‰¹å¾: {len(final_features)}/{len(combined_features)} å¯ç”¨ï¼ˆå«æ™ºèƒ½åŒ¹é…ï¼‰")
    else:
        # ðŸ”§ ä¿®å¤ï¼šå½“æ²¡æœ‰é…ç½®ç‰¹å¾æ—¶ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„éžå…¨NaNç‰¹å¾ï¼ˆè€Œä¸æ˜¯è·³è¿‡è¿‡æ»¤ï¼‰
        print("âš ï¸ æ— ç‰¹å¾é…ç½®ï¼Œä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆç‰¹å¾ï¼ˆåŽ»é™¤å…¨NaNåˆ—ï¼‰")
        # åªä¿ç•™éžå…¨NaNä¸”æ•°æ®è´¨é‡å¥½çš„ç‰¹å¾
        valid_features = []
        for c in unknown_reals:
            if c in available_cols:
                col_data = df_train[c]
                # æ£€æŸ¥åˆ—çš„æœ‰æ•ˆæ€§ï¼šéžå…¨NaNï¼Œä¸”æœ‰æ•ˆå€¼æ¯”ä¾‹>5%
                valid_ratio = col_data.notna().mean()
                if valid_ratio > 0.05:  # è‡³å°‘5%çš„å€¼æ˜¯æœ‰æ•ˆçš„
                    valid_features.append(c)
                else:
                    print(f"  âŒ è·³è¿‡åˆ— {c}: æœ‰æ•ˆå€¼æ¯”ä¾‹ä»… {valid_ratio:.1%}")
        
        unknown_reals = valid_features
        print(f"âœ… è‡ªåŠ¨ç­›é€‰æœ‰æ•ˆç‰¹å¾: {len(unknown_reals)}/{len(unknown_reals_before)}")
    
    # ðŸ§¹ é¢å¤–æ¸…ç†ï¼šç§»é™¤å†…å­˜ä¸­ä¸éœ€è¦çš„å½’ä¸€åŒ–åˆ—
    norm_cols_to_remove = []
    for col in df_train.columns:
        if ("_zn" in col or "_mm" in col) and col not in unknown_reals and col not in known_reals:
            norm_cols_to_remove.append(col)
    
    if norm_cols_to_remove:
        print(f"ðŸ§¹ æ¸…ç†æ— ç”¨å½’ä¸€åŒ–åˆ—: {len(norm_cols_to_remove)} ä¸ª")
        df_train = df_train.drop(columns=norm_cols_to_remove, errors='ignore')
        df_val = df_val.drop(columns=norm_cols_to_remove, errors='ignore')
    # æ±‡æ€»ç‰¹å¾ä¿¡æ¯ï¼Œç”¨äºŽè®­ç»ƒé˜¶æ®µæ‰“å°/è½ç›˜
    features_meta = {
        "selected_allowlist": allowlist,
        "selected_path": sel_path_used,
        "pinned_features": pinned_features,
        "combined_features": combined_features,
        "unknown_reals_before": unknown_reals_before,
        "unknown_reals_after": list(unknown_reals),
        "known_reals": list(known_reals),
        "static_categoricals": list(kwargs.get("static_categoricals", ["symbol", "period"]) or ["symbol", "period"]),
        "static_reals": list(kwargs.get("static_reals", []) or []),
        "nan_fill_summary": nan_fill_summary,
    }
    # æ•°æ®è§„æ¨¡æ‘˜è¦
    def _vc(dff, col):
        try:
            return dff[col].astype(str).value_counts().to_dict()
        except Exception:
            return {}
    features_meta["dataset_summary"] = {
        "total_rows": int(len(df)),
        "train_rows": int(len(df_train)),
        "val_rows": int(len(df_val)),
        "train_by_symbol": _vc(df_train, "symbol"),
        "val_by_symbol": _vc(df_val, "symbol"),
        "train_by_period": _vc(df_train, "period"),
        "val_by_period": _vc(df_val, "period"),
    }

    symbol_classes = sorted(df_train["symbol"].dropna().unique().tolist())
    period_classes = sorted(df_train["period"].dropna().unique().tolist())
    sym2idx = {s: i for i, s in enumerate(symbol_classes)}
    per2idx = {p: i for i, p in enumerate(period_classes)}

    S, P, T = len(symbol_classes), len(period_classes), len(regression_targets)
    means = np.zeros((S, P, T), dtype="float32")
    stds = np.ones((S, P, T), dtype="float32")
    for (sym, per), g in df_train.groupby(["symbol", "period"]):
        si, pi = sym2idx[sym], per2idx[per]
        for ti, col in enumerate(regression_targets):
            if col in g.columns and len(g[col].dropna()):
                m = float(g[col].mean())
                s = float(g[col].std() or 0.0)
                stds[si, pi, ti] = max(s, 1e-8)
                means[si, pi, ti] = m

    norm_pack = {
        "regression_targets": regression_targets,
        "symbol_classes": symbol_classes,
        "period_classes": period_classes,
        "sym2idx": sym2idx,
        "per2idx": per2idx,
        "means": means,
        "stds": stds,
    }

    # No placeholder NaN class: we already drop rows with missing group keys.
    symbol_encoder = NaNLabelEncoder(add_nan=False)
    symbol_encoder.fit(pd.Series(symbol_classes))
    period_encoder = NaNLabelEncoder(add_nan=False)
    period_encoder.fit(pd.Series(period_classes))

    # ç›®æ ‡å½’ä¸€åŒ–å™¨ï¼šå•ç›®æ ‡ç”¨å•ä¸ª Normalizerï¼Œå¤šç›®æ ‡ç”¨ MultiNormalizer
    if len(targets) == 1:
        target_normalizer = TorchNormalizer(method="identity", center=False)
        ts_target = targets[0]
    else:
        target_normalizer = MultiNormalizer([TorchNormalizer(method="identity", center=False)] * len(targets))
        # å•ç›®æ ‡æ—¶ä½¿ç”¨å­—ç¬¦ä¸²ç›®æ ‡ï¼Œå¤šç›®æ ‡æ—¶ä½¿ç”¨åˆ—è¡¨
        ts_target = targets
    # å…è®¸é€šè¿‡å¯é€‰å‚æ•°è¦†ç›– TimeSeriesDataSet å…³é”®é…ç½®
    max_encoder_length = int(kwargs.get("max_encoder_length", 36))
    max_prediction_length = int(kwargs.get("max_prediction_length", 1))
    add_relative_time = bool(kwargs.get("add_relative_time_idx", True))
    add_target_scales = bool(kwargs.get("add_target_scales", False))
    allow_missing = bool(kwargs.get("allow_missing_timesteps", True))
    static_cats = kwargs.get("static_categoricals", ["symbol", "period"]) or ["symbol", "period"]
    static_reals_cfg = kwargs.get("static_reals", []) or []

    ts_cfg = dict(
        time_idx="time_idx",
        target=ts_target,
        group_ids=["symbol", "period"],
        max_encoder_length=max_encoder_length,
        max_prediction_length=max_prediction_length,
        static_categoricals=static_cats,
        static_reals=static_reals_cfg,
        categorical_encoders={
            "symbol": symbol_encoder,
            "period": period_encoder,
        },
        time_varying_known_reals=known_reals,
        time_varying_unknown_reals=unknown_reals,
        add_relative_time_idx=add_relative_time,
        add_target_scales=add_target_scales,
        allow_missing_timesteps=allow_missing,
        target_normalizer=target_normalizer,
    )

    train_ds = TimeSeriesDataSet(df_train, **ts_cfg)
    val_ds = TimeSeriesDataSet.from_dataset(train_ds, df_val, stop_randomization=True)

    train_loader = train_ds.to_dataloader(
        train=True,
        batch_size=batch_size,
        num_workers=num_workers,
        persistent_workers=num_workers > 0,
        shuffle=True,
        pin_memory=True,
        prefetch_factor=4,
        drop_last=True,
    )
    # éªŒè¯é›†ä¸è¦ä¸¢å¼ƒæœ€åŽä¸€ä¸ªä¸æ»¡æ‰¹æ¬¡ï¼Œé¿å…æ ·æœ¬è¿‡å°‘æ—¶åº¦é‡å™¨æ— æ ·æœ¬
    val_loader = val_ds.to_dataloader(
        train=False,
        batch_size=batch_size,
        num_workers=num_workers,
        persistent_workers=num_workers > 0,
        pin_memory=True,
        prefetch_factor=2,
        drop_last=False,
    )
    return train_loader, val_loader, targets, train_ds, period_classes, norm_pack, features_meta
