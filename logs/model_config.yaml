accumulate: 2
attention_head_size: 8
batch_size: 1280
data_path: data/pkl_merged/full_merged.pkl
dropout: 0.2
early_stop_patience: 5
grad_clip: 0.2
hidden_size: 192
learning_rate: 0.0759
log_name: tft_multi
loss_schedule:
  0:
  - 6.0
  - 3.0
  - 4.0
  - 1.5
  - 3.5
  - 2.5
  - 1.5
  - 1.2
  - 1.2
  - 1.8
  - 1.3
  5:
  - 5.0
  - 3.0
  - 3.5
  - 1.5
  - 3.0
  - 2.0
  - 1.5
  - 1.0
  - 1.0
  - 1.5
  - 1.0
lstm_layers: 2
max_epochs: 3
num_workers: 15
precision: 16-mixed
resume_ckpt: checkpoints/last.ckpt
val_days: 14
val_mode: ratio
val_ratio: 0.2
warm_start_ckpt: checkpoints/best.ckpt
