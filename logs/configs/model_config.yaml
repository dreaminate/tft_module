accumulate: 2
attention_head_size: 8
batch_size: 64
data_path: data/parquet_merged/full_merged.parquet
dropout: 0.15
early_stop_patience: 5
focus_period: None
grad_clip: 0.2
hidden_size: 128
learning_rate: 0.003
log_name: tft_multi
lstm_layers: 2
max_epochs: 20
num_workers: 4
precision: 16-mixed
resume_ckpt: checkpoints/tft-last.ckpt
sampler_mode: None
val_days: 14
val_mode: ratio
val_ratio: 0.1
warm_start_ckpt: checkpoints/tft-best.ckpt
